{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f413cfc-e141-474b-90c3-77e4646b21bd",
   "metadata": {},
   "source": [
    "# Program 4: NYC Taxi Rides"
   ]
  },
  {
   "cell_type": "code",
   "id": "bb7872ff-990a-4487-a93a-dbbaf452d33d",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "Name: Jose Miguel Ortiz\n",
    "Email: jose.ortiz60@lagcc.cuny.edu\n",
    "\"\"\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "732123b4-907f-435e-aa03-154e29bc24a2",
   "metadata": {},
   "source": [
    "## Section 0: Import objects and define utility functions\n",
    "Take a look at these imports. You'll be using all of these modules, functions, and classes!\n",
    "\n",
    "As you work through this notebook, make sure to consult the [Scikit Learn's API documentation](https://scikit-learn.org/stable/api/index.html) to understand how different functions and objects work."
   ]
  },
  {
   "cell_type": "code",
   "id": "3a8c7931-835d-4f2e-9c43-cd9ece1761c2",
   "metadata": {},
   "source": [
    "# data wrangling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data visualization\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# prediction models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# model selection and model evaluation\n",
    "from sklearn.model_selection import train_test_split, cross_validate, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, make_scorer\n",
    "\n",
    "# feature engineering and preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# sklearn task pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "cee5ea6d-9d00-4b9e-8193-65d38adec960",
   "metadata": {},
   "source": [
    "# utility function\n",
    "def classification_scores(y_true, y_pred, print_scores=False, round_precision=4):\n",
    "    \"\"\"Compute (and optionally print) common classification scores.\n",
    "\n",
    "    :param y_true: observed values.\n",
    "    :type y_true: pandas.Series, list, numpy.ndarray or some other collection.\n",
    "    :param y_pred: predicted values.\n",
    "    :type y_true: pandas.Series, list, numpy.ndarray or some other collection.\n",
    "    :param print_scores: flag to print scores to stdout. Defaults to False.\n",
    "    :type print_scores: bool.\n",
    "    :param round_precision: number of decimals for round function. Defaults to 4.\n",
    "    :type round_precision: int.\n",
    "\n",
    "    :returns: accuracy, precision, recall, and confusion matrices.\n",
    "    :rtype: dict.\n",
    "    \"\"\"\n",
    "\n",
    "    scores = {}\n",
    "    scores[\"accuracy\"] = accuracy_score(y_true, y_pred).round(round_precision)\n",
    "    scores[\"precision\"] = precision_score(y_true, y_pred).round(round_precision)\n",
    "    scores[\"recall\"] = recall_score(y_true, y_pred).round(round_precision)\n",
    "    scores[\"confusion_matrix\"] = confusion_matrix(y_true, y_pred)\n",
    "    scores[\"normalized_confusion_matrix\"] = confusion_matrix(y_true, y_pred, normalize=\"all\").round(round_precision)\n",
    "\n",
    "    if print_scores:\n",
    "        print(\"------------------------------------------------------\")\n",
    "        print(\"               CLASSIFICATION SCORES                  \")\n",
    "        print(\"------------------------------------------------------\")\n",
    "        print(f\"                   accuracy: \", scores[\"accuracy\"])\n",
    "        print(f\"                  precision: \", scores[\"precision\"])\n",
    "        print(f\"                     recall: \", scores[\"recall\"])\n",
    "        print(f\"           confusion matrix: \", scores[\"confusion_matrix\"][0, :])\n",
    "        print(f\"                             \", scores[\"confusion_matrix\"][1, :])\n",
    "        print(f\"normalized confusion matrix: \", scores[\"normalized_confusion_matrix\"][0, :])\n",
    "        print(f\"                             \", scores[\"normalized_confusion_matrix\"][1, :])\n",
    "        print(\"------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    return scores"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "74e4be03-b4a1-4e72-811f-777841cd1fbc",
   "metadata": {},
   "source": [
    "## Section 1: Prepare data\n",
    "\n",
    "### Task 1.1: Load the dataset\n",
    "Use seaborn's `load_dataset()` function to import the NYC taxi ride dataset. Store the output dataframe as a `taxi_rides` object."
   ]
  },
  {
   "cell_type": "code",
   "id": "01656058-6be4-473d-9866-6605198a1795",
   "metadata": {},
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # load taxi rides dataset\n",
    "    taxi_rides = sns.load_dataset(\"taxis\")\n",
    "    taxi_rides.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cf83f2ef-e4f2-4f27-af13-c649f8906aa1",
   "metadata": {},
   "source": [
    "### Task 1.2: Create target variable\n",
    "You're going to build classification models to predict whether a ride will have a tip or not. Create a new column called `\"tipped\"` in your `taxi_rides` dataframe that is `True` if the tip amount is larger or equal than `0.25` and `False` otherwise. That way, _nominal_ tips (e.g. 1Â¢) will be considered as no tip."
   ]
  },
  {
   "cell_type": "code",
   "id": "c066b2da-2dfd-41f1-bf3e-30312c9c14a9",
   "metadata": {},
   "source": "taxi_rides[\"tipped\"] = taxi_rides[\"tip\"] >= 0.25",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "81f2ec82-470d-492f-87be-50b7c091c129",
   "metadata": {},
   "source": [
    "### Task 1.3: Split data into train and test subsets\n",
    "\n",
    "First, create two new objects `X` and `y` containing the feature and target columns respectively. Then, use scikit-learn's `train_test_split()` function to separate out the train and test data. Make sure to set \n",
    "\n",
    "- the test size to 20%\n",
    "- the random seed to `42`"
   ]
  },
  {
   "cell_type": "code",
   "id": "dd59d81b-f1b2-4c85-8a0e-f5726af5b414",
   "metadata": {},
   "source": [
    "# separate target (y) and feature (X) variables \n",
    "y = taxi_rides[\"tipped\"]\n",
    "X = taxi_rides.drop(\"tipped\", axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4142609a-0d1c-4684-90aa-ccd9a5c596bd",
   "metadata": {},
   "source": [
    "# split into train and test data subsets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3af6a175-66df-40ca-b73f-00e6a85bde1b",
   "metadata": {},
   "source": [
    "### Task 1.4: Transformer for numeric features\n",
    "Now that we have split out data into train and test, features and target, we need to set up the preprocessing steps that the feature columns will need before we can train a model.\n",
    "\n",
    "Start by creating a `numeric_features` list with the numeric columns. Then, create an instance of the scikit-learn `Pipeline` object called `numeric_transformer` with two steps: an `\"imputer\"` step to impute any missing values with each columns median, and a `\"scaler\"` step to standardize the variables after imputation. Then, visualize a graphical representation of the tranformer. Finally, call `.fit_transform()` on the numeric columns of `X_train` to ensure it works."
   ]
  },
  {
   "cell_type": "code",
   "id": "6c473f32-3780-48a7-b170-1495f98a3559",
   "metadata": {},
   "source": [
    "# create list with numeric feature/column names\n",
    "numeric_features = [\n",
    "    \"passengers\", \n",
    "    \"distance\", \n",
    "    \"fare\", \n",
    "    \"tolls\"\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "014f13ef-02c5-45ca-a432-983b52344b79",
   "metadata": {},
   "source": [
    "# pipeline for numeric features\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "31c5fb27-8983-4d05-b3fd-3d69be73aefa",
   "metadata": {},
   "source": [
    "# visualize the transformer\n",
    "numeric_transformer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "74c9467f-9e69-432b-b3bd-9ac69f59b4ba",
   "metadata": {},
   "source": [
    "# execute the transformer to ensure it works\n",
    "numeric_transformer.fit_transform(X_train[numeric_features])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7f77af73-cb99-4173-affd-65002a7e2859",
   "metadata": {},
   "source": [
    "### Task 1.5: transformer for categorical features\n",
    "This task is analogous to the previous one but for the categorical features. Here you'll create a `Pipeline` called `categorical_transformer` with two steps: an `\"imputer\"` step that will replace any missing values with the most frequent ones, and an `\"encoder\"` step that will perform one-hot encoding on the categorical variables. Finally, visualize the transformer and check that the transformer works on `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "id": "12d635fb-be67-440e-b7e5-6540b1eefbce",
   "metadata": {},
   "source": [
    "# create list with categorical feature/column names\n",
    "categorical_features = [\n",
    "    \"color\", \n",
    "    \"payment\",\n",
    "    \"pickup_borough\",\n",
    "    \"dropoff_borough\",\n",
    "]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "57e32845-0170-43b4-8a95-7e1249b5134f",
   "metadata": {},
   "source": [
    "# pipeline for categorical features\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "014f8dff-b27f-40c9-a71b-b2f8ccfc80ce",
   "metadata": {},
   "source": [
    "# visualize the transformer\n",
    "categorical_transformer"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9e0c8fe6-8f77-4144-a4d6-3859d019bdc7",
   "metadata": {},
   "source": [
    "# execute the transformer to ensure it works\n",
    "categorical_transformer.fit_transform(X_train[categorical_features]).todense()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "65f0da28-4255-461d-8171-36e463c98031",
   "metadata": {},
   "source": [
    "### Task 1.6: combine transformers into a single preprocessor\n",
    "Now that you've got transformers for numeric and categorical features, you'll combine them into a single object so you can perform transformations in one operation.\n",
    "\n",
    "Create a `ColumnTransformer` object and store it as `preprocessor`. It will have the two transformers you defined above, labeled `\"numeric\"`, and `\"categorical\"` respectively. Then, visualize the preprocessor."
   ]
  },
  {
   "cell_type": "code",
   "id": "8d48a56c-b21f-4809-9ccb-cd7b62968cff",
   "metadata": {},
   "source": [
    "# combine both pipelines into a single transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"numeric\", numeric_transformer, numeric_features),\n",
    "        (\"categorical\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e924f3b3-267c-4fda-acad-54de9a729d32",
   "metadata": {},
   "source": [
    "# visualize the preprocessor\n",
    "preprocessor"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "41067ea3-9dd1-4b32-9fef-406060e38c50",
   "metadata": {},
   "source": [
    "### Task 1.7: check the preprocessor works\n",
    "This is our chance to ensure we're happy with our `preprocessor` before we start building models. \n",
    "\n",
    "First create a dataframe called `X_train_transformed` with both numeric and categorical features. Then, use the dataframe's `describe()` method to show a summary of all the transformed features. Then you'll write out some observations on the transformed data (details below)."
   ]
  },
  {
   "cell_type": "code",
   "id": "a2761810-7c45-4fce-9743-260ee83dcbc5",
   "metadata": {},
   "source": [
    "# fit and transform the preprocessor on X_train\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# arrange in a dataframe\n",
    "X_train_transformed = pd.DataFrame(\n",
    "    data=X_train_transformed,\n",
    "    columns=preprocessor.get_feature_names_out()\n",
    ")\n",
    "\n",
    "# display the dataframe\n",
    "X_train_transformed"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "34d64cfd-bebc-4797-be17-76cb563a57f5",
   "metadata": {},
   "source": [
    "# compute a summary of all the features\n",
    "X_train_transformed.describe()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "What stands out to you as you observed the transformed data, both disaggregated and summarized? Write at least two observations.\n",
    "> - 1.7 #1: After transformation, all numeric features (passengers, distance, fare, tolls) have been standardized to have mean=0 and standard deviation=1, which puts them on a comparable scale for modeling.\n",
    "> - 1.7 #2: The categorical features have been one-hot encoded, creating binary columns for each category (e.g. color_green, color_yellow, payment_cash, etc.) which allows these nominal variables to be used in machine learning models.\n",
    "> - 1.7 #3: Some numeric features like 'fare' and 'distance' show significant variability in their distributions, with outliers present even after standardization.\n"
   ],
   "id": "292681b31f255110"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "def479b8cdce6426",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fa5256f6-c5f4-433e-8a8a-f8a218187ce2",
   "metadata": {},
   "source": [
    "## Section 2: Vanilla logistic regression model\n",
    "\n",
    "Here we're going to be building a simple logistic regression model to predict whether or not a trip will have a tip. For now we will not worry about tuning the logistic regression decision threshold, nor the L1 or L2 regularization strengths. We first want to make sure we can assemble a scikit-learn pipepline that can take `X_train`, and `y_train` and return a fitted model.\n",
    "\n",
    "### Task 2.1 Build the model pipeline\n",
    "A very nice feature of scikit-learn pipeplines is that they can contain other pipelines. You'll leverage that here, building a new pipepline with two steps: a `\"preprocessor\"` step with the `preprocessor` object you defined above, and a `\"lr\"` step with a `LogisticRegression()` model with default parameters. Call your pipeline `vanilla_lr`. Then, fit it on your train data.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b2c7b16b-5673-46a6-85aa-3f50c09a3439",
   "metadata": {},
   "source": [
    "# build a pipeline with preprocessor and vanilla LR\n",
    "vanilla_lr = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"lr\", LogisticRegression())\n",
    "])\n",
    "\n",
    "# train the pipeline\n",
    "vanilla_lr.fit(X_train, y_train)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f33df4af-c658-47e8-b55b-d457e5aa73f6",
   "metadata": {},
   "source": [
    "### Task 2.2: Make predictions\n",
    "Your model is trained so you can use it to make predictions now. Make predictions for both train and test sets, calling the outputs `y_train_vanilla_lr` and `y_test_vanilla_lr` respectively. Then, call the `classification_scores()` utility function defined in **Section 0** to print the classification scores for both the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "id": "d0f3f972-54d9-4284-bf85-5fa099bb98d8",
   "metadata": {},
   "source": [
    "# make predictions for train and test data\n",
    "y_train_vanilla_lr = vanilla_lr.predict(X_train)\n",
    "y_test_vanilla_lr = vanilla_lr.predict(X_test)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # compute and print the classification scores for test data\n",
    "    def classification_scores(y_true, y_pred, print_scores=False, round_precision=4):\n",
    "        \"\"\"Compute (and optionally print) common classification scores.\"\"\"\n",
    "\n",
    "        scores = {}\n",
    "        scores[\"accuracy\"] = round(accuracy_score(y_true, y_pred), round_precision)\n",
    "        scores[\"precision\"] = round(precision_score(y_true, y_pred, zero_division=0), round_precision)\n",
    "        scores[\"recall\"] = round(recall_score(y_true, y_pred, zero_division=0), round_precision)\n",
    "        scores[\"confusion_matrix\"] = confusion_matrix(y_true, y_pred)\n",
    "        scores[\"normalized_confusion_matrix\"] = np.round(\n",
    "            confusion_matrix(y_true, y_pred, normalize=\"all\"), round_precision\n",
    "        )\n",
    "\n",
    "        if print_scores:\n",
    "            print(\"------------------------------------------------------\")\n",
    "            print(\"               CLASSIFICATION SCORES                  \")\n",
    "            print(\"------------------------------------------------------\")\n",
    "            print(f\"                   accuracy: {scores['accuracy']}\")\n",
    "            print(f\"                  precision: {scores['precision']}\")\n",
    "            print(f\"                     recall: {scores['recall']}\")\n",
    "            print(f\"           confusion matrix: {scores['confusion_matrix'][0, :]}\")\n",
    "            print(f\"                             {scores['confusion_matrix'][1, :]}\")\n",
    "            print(f\"normalized confusion matrix: {scores['normalized_confusion_matrix'][0, :]}\")\n",
    "            print(f\"                             {scores['normalized_confusion_matrix'][1, :]}\")\n",
    "            print(\"------------------------------------------------------\")\n",
    "\n",
    "        return scores"
   ],
   "id": "7b13ef96059dbd1b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 2.3: Analyze classification scores\n",
    "\n",
    "Compare the results for the train data and for the holdout test data. What stands out to you? Write at least two observations.\n",
    "\n",
    "> - 2.3 #1: The model performs slightly better on the training data than on the test data, which is expected due to overfitting. However, the difference is not very large, suggesting that the model generalizes reasonably well.\n",
    "> - 2.3 #2: The precision is higher than the recall for both train and test data, indicating that the model is more conservative in predicting positive cases (tipped rides). This means it's more likely to miss some tipped rides (false negatives) than to incorrectly classify non-tipped rides as tipped (false positives).\n",
    "> - 2.3 #3: The confusion matrix shows that the model is better at predicting one class over the other, which could be due to class imbalance in the dataset."
   ],
   "id": "1250bc778a23bebc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 3: Cross validation\n",
    "In Section 2 you built one model. Now you'll build many models in sequence to quantify how well your model generalizes to independent data.\n",
    "\n",
    "### Task 3.1: Perform 10-fold CV on your vanilla logistic regressor\n",
    "Use the `cross_validate()` scikit-learn function to perform 10-fold cross validation on your train data. You'll want to make sure to pass the following arguments:\n",
    "- `estimator`: your vanilla logistic regression estimator\n",
    "- `X`: your input features\n",
    "- `y`: your input target\n",
    "- `scoring`: a dictionary of score names and scikit-learn scorer functions. Compute the most common classification metrics: accuracy, precision, and recall.\n",
    "- `return_train_score`: a boolean flag to get the train data scores back. Set this to `True`.\n",
    "\n",
    "Store the output of your `cross_validate(...)` call into `cv_results`."
   ],
   "id": "358aecca6dcee040"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "cv_results = cross_validate(\n",
    "    estimator=vanilla_lr,\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    cv=10,\n",
    "    scoring={\n",
    "        \"accuracy\": make_scorer(accuracy_score),\n",
    "        \"precision\": make_scorer(precision_score),\n",
    "        \"recall\": make_scorer(recall_score)\n",
    "    }, \n",
    "    return_train_score=True\n",
    ")"
   ],
   "id": "dbd5296c761f9b81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 3.2: Plot train and test scores\n",
    "\n",
    "Convert `cv_results` into a dataframe `cv_results_df` and display it. Then create a matplotlib subplots object with 3 columns, and make box plots for the train and test scores across the 10 folds. Finally, write at least two observations about these results."
   ],
   "id": "afb9a6f65d9d4c2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# convert CV results into a dataframe\n",
    "cv_results = pd.DataFrame(cv_results)\n",
    "\n",
    "# display the dataframe\n",
    "cv_results"
   ],
   "id": "8ba7ae2a2650582",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# create subplots and make a box plot for each score\n",
    "fig, ax = plt.subplots(ncols=3, figsize=(15, 3))\n",
    "\n",
    "sns.boxplot(\n",
    "    data=cv_results.melt(value_vars=[\"train_accuracy\", \"test_accuracy\"]),\n",
    "    x=\"variable\",\n",
    "    y=\"value\",\n",
    "    ax=ax[0]\n",
    ")\n",
    "\n",
    "sns.boxplot(\n",
    "    data=cv_results.melt(value_vars=[\"train_precision\", \"test_precision\"]),\n",
    "    x=\"variable\",\n",
    "    y=\"value\",\n",
    "    ax=ax[1]\n",
    ")\n",
    "\n",
    "sns.boxplot(\n",
    "    data=cv_results.melt(value_vars=[\"train_recall\", \"test_recall\"]),\n",
    "    x=\"variable\",\n",
    "    y=\"value\",\n",
    "    ax=ax[2]\n",
    ");"
   ],
   "id": "712ebda32492f407",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Write at least two observations or conclusions about the data/plots here.\n",
    "> - 3.2 #1: The train scores are consistently higher than the test scores across all metrics (accuracy, precision, recall), which indicates some degree of overfitting. However, the gap between train and test scores is not very large, suggesting that the model still generalizes reasonably well.\n",
    "> - 3.2 #2: The variance in test scores across the 10 folds is higher than the variance in train scores, which is expected. This indicates that the model's performance is somewhat sensitive to the specific data split used for evaluation.\n",
    "> - 3.2 #3: The precision scores are generally higher than the recall scores, suggesting that the model is better at avoiding false positives than false negatives. This means it's more conservative in predicting positive cases (tipped rides)."
   ],
   "id": "f649ccc47c7492d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 3.3: Manually optimizing the model hyperparameters with cross validation\n",
    "Here, instead of using logistic regression you're going to build a `KNeighborsClassifier`. This classifier has a couple of interesting hyperparameters: \n",
    "- `n_neighbors`: the number of nearest neighbors to compute a prediction\n",
    "- `p`: the power of the distance function (`2` for Euclidean, `1` for Manhattan, etc.)\n",
    "\n",
    "For now, fix `p` to `2` (the default value) and vary `n_neighbors` over `range(1, 30, 2)`. In each step, build a pipeline called `cv_knc` with the `preprocessor` you built earlier, and another step called `\"knc\"` with your `KNeighborsClassifier`. Then run `cross_validate()` on your train data just like above in this section. Finally, convert the results to a dataframe, add `cv_split` and `n_neighbors` columns to the table for that iteration, and concatenate it to `cv_results_all`."
   ],
   "id": "2aabf0d98ff583aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# this may take a minute, you're training 15*10 models!\n",
    "cv_results_all = pd.DataFrame()\n",
    "n_neighbors_all = range(1, 30, 2)\n",
    "\n",
    "for n_neighbors in n_neighbors_all:\n",
    "\n",
    "    cv_knc = Pipeline(steps=[\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"knc\", KNeighborsClassifier(n_neighbors=n_neighbors, p=2))\n",
    "    ])\n",
    "\n",
    "    cv_results = cross_validate(\n",
    "        estimator=cv_knc, \n",
    "        X=X_train,\n",
    "        y=y_train,\n",
    "        cv=10,\n",
    "        scoring={\n",
    "            \"accuracy\": make_scorer(accuracy_score),\n",
    "            \"precision\": make_scorer(precision_score),\n",
    "            \"recall\": make_scorer(recall_score)\n",
    "        },\n",
    "        return_train_score=True\n",
    "    )\n",
    "\n",
    "    cv_results = pd.DataFrame(cv_results)\n",
    "    cv_results.index.name = \"cv_split\"\n",
    "    cv_results[\"n_neighbors\"] = n_neighbors\n",
    "    cv_results = cv_results.reset_index()\n",
    "\n",
    "    cv_results_all = pd.concat([cv_results_all, cv_results])\n",
    "\n",
    "cv_results_all = cv_results_all.reset_index()\n",
    "\n",
    "# display\n",
    "cv_results_all"
   ],
   "id": "a173699920c20046",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 3.4: Plot scores and analyze\n",
    "Make lineplots with the CV train and test scores for all 3 metrics as a function of `n_neighbors` and analyze the results."
   ],
   "id": "126fe8015181b513"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot classification metrics for CV train and test sets\n",
    "fig, ax = plt.subplots(ncols=3, figsize=[15, 3])\n",
    "\n",
    "sns.lineplot(\n",
    "    data=cv_results_all,\n",
    "    x=\"n_neighbors\",\n",
    "    y=\"train_accuracy\",\n",
    "    label=\"train\",\n",
    "    ax=ax[0]\n",
    ")\n",
    "\n",
    "sns.lineplot(\n",
    "    data=cv_results_all,\n",
    "    x=\"n_neighbors\",\n",
    "    y=\"test_accuracy\",\n",
    "    label=\"test\",\n",
    "    ax=ax[0]\n",
    ")\n",
    "\n",
    "sns.lineplot(\n",
    "    data=cv_results_all,\n",
    "    x=\"n_neighbors\",\n",
    "    y=\"train_precision\",\n",
    "    label=\"train\",\n",
    "    ax=ax[1]\n",
    ")\n",
    "\n",
    "sns.lineplot(\n",
    "    data=cv_results_all,\n",
    "    x=\"n_neighbors\",\n",
    "    y=\"test_precision\",\n",
    "    label=\"test\",\n",
    "    ax=ax[1]\n",
    ")\n",
    "\n",
    "sns.lineplot(\n",
    "    data=cv_results_all,\n",
    "    x=\"n_neighbors\",\n",
    "    y=\"train_recall\",\n",
    "    label=\"train\",\n",
    "    ax=ax[2]\n",
    ")\n",
    "\n",
    "sns.lineplot(\n",
    "    data=cv_results_all,\n",
    "    x=\"n_neighbors\",\n",
    "    y=\"test_recall\",\n",
    "    label=\"test\",\n",
    "    ax=ax[2]\n",
    ")\n",
    "\n",
    "ax[0].set_ylabel(\"accuracy\")\n",
    "ax[1].set_ylabel(\"precision\")\n",
    "ax[2].set_ylabel(\"recall\");"
   ],
   "id": "ef7e3efd0c8b77a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "What observations do you make? What value of `n_neighbors` would you choose if your score of interest is accuracy?\n",
    "> - 3.4 #1: As n_neighbors increases, the train accuracy decreases while the test accuracy initially increases and then plateaus or slightly decreases. This is because with a small number of neighbors, the model tends to overfit to the training data, but with too many neighbors, it becomes too generalized and loses predictive power.\n",
    "> - 3.4 #2: The gap between train and test accuracy is largest when n_neighbors is small (especially at n_neighbors=1), which indicates overfitting. As n_neighbors increases, this gap narrows, suggesting better generalization.\n",
    "> - 3.4 #3: Based on the test accuracy curve, I would choose n_neighbors around 9-13, where the test accuracy appears to reach its peak before starting to decline. This represents a good balance between model complexity and generalization ability."
   ],
   "id": "7df5e303ef8d3a3c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Section 4: Grid search cross validation\n",
    "In this section you'll build a more powerful and robust method to optimize your model hyperparameters: grid search. You will perform 10-fold cross validation at multiple nodes of a 2-dimensional grid of hyperparameters, searching the optimal combination of `n_neighbors` and `p`.\n",
    "\n",
    "### Task 5.1: Build and fit a grid search pipeline\n",
    "Create a new `Pipeline` object called `knc` with two steps:\n",
    "1. `\"preprocessor\"`: the same preprocessor you've been using\n",
    "2. `\"knc\"`: a `KNeighborsClassifier` instance.\n",
    "\n",
    "Then, instantiate an object of the scikit-learn's `GridSearchCV` class passing the following arguments:\n",
    "- `estimator`: your `knc` pipeline\n",
    "- `param_grid`: the following (hyper)parameter grid dictionary\n",
    "  ```python\n",
    "    {\n",
    "        \"knc__n_neighbors\": range(1, 25),\n",
    "        \"knc__p\": [1, 2],\n",
    "    }\n",
    "  ```\n",
    "- `scoring`: accuracy score function\n",
    "- `cv`: the number of folds\n",
    "- `return_train_score=True`\n",
    "\n",
    "Store the object as `gs_knc`. Then, fit the pipeline on your train dataset."
   ],
   "id": "a2d63c2768500786"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "knc = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"knc\", KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "gs_knc = GridSearchCV(\n",
    "    estimator=knc,\n",
    "    param_grid={\n",
    "        \"knc__n_neighbors\": range(1, 25),\n",
    "        \"knc__p\": [1, 2],\n",
    "    },\n",
    "    scoring=\"accuracy\",\n",
    "    cv=10,\n",
    "    return_train_score=True\n",
    ")\n",
    "\n",
    "gs_knc.fit(X_train, y_train)"
   ],
   "id": "91e76a3f1812be67",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 5.2: find the optimal parameters and show the grid search results\n",
    "First, print the `best_params_` attribute of `gs_knc`. Then, make a lineplots of the accuracy vs `n_neighbors` for both values of `p`, as well as both for CV train and test data."
   ],
   "id": "c0e55a81afb51b7a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print the best hyperparameters\n",
    "print(\"Best parameters:\", gs_knc.best_params_)"
   ],
   "id": "ba6563ae5e5f0856",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot the results\n",
    "_plot_data =  pd.DataFrame(gs_knc.cv_results_).melt(\n",
    "    id_vars=[\"param_knc__n_neighbors\", \"param_knc__p\"], \n",
    "    value_vars=[\"mean_train_score\", \"mean_test_score\"]\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "sns.lineplot(\n",
    "    data=_plot_data,\n",
    "    x=\"param_knc__n_neighbors\",\n",
    "    y=\"value\",\n",
    "    style=\"variable\",\n",
    "    hue=\"param_knc__p\",\n",
    "    palette=\"Set1\"\n",
    ");"
   ],
   "id": "17a9ef34b7f1347c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Task 5.3: Compute your final metrics and discuss the results.\n",
    "One of the nice features of `GridSearchCV` is that it will automatically train your estimator on the entire training set for the optimal hyperparameter values. That allows us to make predictions directly with the `gs_knc` object.\n",
    "\n",
    "Compute the accuracy score on your train and test datasets. Then discuss the results of this entire section."
   ],
   "id": "9c11048057eaf14e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# compute accuracy score on train dataset\n",
    "accuracy_score(y_train, gs_knc.predict(X_train))"
   ],
   "id": "145ddabdc5abb94f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# compute accuracy score on test dataset\n",
    "accuracy_score(y_test, gs_knc.predict(X_test))"
   ],
   "id": "bb6a4e23a28a0f53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Write at least two observations or conclusions about the results and plot you got in this section.\n",
    "> - 5.3 #1: The grid search found the optimal hyperparameters for the KNN model, which likely improved the model's performance compared to the default parameters. The accuracy on the test set is a good indicator of how well the model will generalize to new, unseen data.\n",
    "> - 5.3 #2: The plot shows that the Manhattan distance metric (p=1) generally performs better than the Euclidean distance metric (p=2) for this dataset, especially for larger values of n_neighbors. This suggests that the features in this dataset may have different scales or contain outliers, which the Manhattan distance is more robust against.\n",
    "> - 5.3 #3: The grid search approach is more systematic and comprehensive than manual hyperparameter tuning, as it explores all combinations of hyperparameters in the specified grid. This increases the chances of finding the optimal configuration, although it can be computationally expensive for large grids or datasets.\n"
   ],
   "id": "24804613ddf39e92"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

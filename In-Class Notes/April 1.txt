April 1, 2025 Class Notes:

- Quick note on logistic regression
In multiple linear regression we write
y (x) = Theta ·x
where (Theta)θ·x = θ0 + θ1x1 + θ2x2 +···
and for logistic regression we have
y (x) = 1
1 + exp (Theta)θ·x
So generally we can write
y (x) = η( (Theta)θ·x)

-----------------------------------------------------------------------------
#NOTES:


random forests
XGBoost

Neural Networks

- Machine Learning:

Finding patterns in data is where machine learning comes in. Machine learning methods use statistical learning to identify boundaries.
One example of a machine learning method is a decision tree.
Decision trees look at one variable at a time and are a reasonably accessible (though rudimentary) machine learning method.

In machine learning, these statements are called forks, and they split the data into two branches based on some value.

That value between the branches is called a split point. Homes to the left of that point get categorized in one way, while those to the right are categorized in another.
A split point is the decision tree's version of a boundary.


-- Tradeoffs

Picking a split point has tradeoffs. Our initial split (~73 m) incorrectly classifies some San Francisco homes as New York ones.

Look at that large slice of green in the left pie chart, those are all the San Francisco homes that are misclassified.
These are called false negatives.

However, a split point meant to capture every San Francisco home will include many New York homes as well. These are called false positives.

• The ultimate branches of the tree are called leaf nodes.
    Our decision tree models will classify the homes in each leaf node according to which class of homes is in the majority.


• Recap:

    - Machine learning identifies patterns using statistical learning and computers by unearthing boundaries in data sets. You can use it to make predictions.
    - One method for making predictions is called a decision trees, which uses a series of if-then statements to identify boundaries and define patterns in the data.
    - Overfitting happens when some boundaries are based on on distinctions that don't make a difference. You can see if a model overfits by having test data flow through the model.



---------------------------------------------------------------------------


• GLM (Generalized Linear Model)

- logistic regression:

    Theta • |X (sample mean) = X

• Classification Survey:

    Goal: classify red versus blue points.
    Many different alternatives...
        ▶ Discriminative: model conditional probability P(y |x)
            ⋆ Logistic regression.
            ⋆ Support vector machines: separate (transformed) space with a plane.
            ⋆ Feed-forward neural networks.
        ▶ Generative: model joint probability P(y ,x)
            ⋆ Naive Bayes models: quick, few parameters.
            ⋆ Generative adversarial networks.
        ▶ Ensemble methods: combine multiple classifiers into one.
            ⋆ Random forests: combines decision trees on input data subspaces.
            ⋆ XGBoost: combines a large number of weak, shallow trees.

• Decision Trees:

    Ensemble method: aggregates the results of simpler estimators.
    The simple estimators are decisions trees.
    The ensemble is a random forest.
    Does surprisingly well: a majority vote of estimators can be better than any of the
    estimators on their own.


•




• Multiclass classification:

    Multiclass classification: more than 2 classes.
    One-vs-all: train a binary classifier for each class.
        ▶ For each class, train a classifier to distinguish that class from all others.
        ▶ For each test point, classify it with the classifier that gives the highest score.
    One-vs-one: train a binary classifier for each pair of classes.
        ▶ For each pair of classes, train a classifier to distinguish between them.
        ▶ For each test point, classify it with the classifier that gives the highest score.


------

• Dimensionality Reduction:
    Dimensionality reduction techniques are used for analysis & visualization.

    Principal Components Analysis:
        ▶ Transforms data so that variation is on fewest coordinates. First coordinate has most
    variation, second has next most, etc.
        ▶ Use first 2 (or 3) components for visualization.
    Multidimensional Scaling:
        ▶ Preserve relative distances in the mapping.
        ▶ Common criteria: minimize MSE between true distances and screen distances.